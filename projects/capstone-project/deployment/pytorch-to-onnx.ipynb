{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx\n",
      "  Downloading onnx-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (7.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting onnxruntime\n",
      "  Downloading onnxruntime-1.3.0-cp37-cp37m-manylinux1_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 18.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from onnx) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/conda/lib/python3.7/site-packages (from onnx) (3.7.4.2)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from onnx) (3.11.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from onnx) (1.18.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf->onnx) (47.1.1.post20200529)\n",
      "Installing collected packages: onnx, onnxruntime\n",
      "Successfully installed onnx-1.7.0 onnxruntime-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dog Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_detection_model = torch.load('dog_detection_model.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_detection_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to the model\n",
    "BATCH_SIZE =  1\n",
    "x = torch.randn(BATCH_SIZE, 3, 224, 224, requires_grad=True, device='cpu')\n",
    "output = dog_detection_model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(dog_detection_model,\n",
    "                  x,\n",
    "                  \"dog-detection-model.onnx\",\n",
    "                  export_params=True,\n",
    "                  opset_version=11,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names=['input'],\n",
    "                  output_names=['output']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph torch-jit-export (\\n  %input[FLOAT, 1x3x224x224]\\n) initializers (\\n  %classifier.1.bias[FLOAT, 1000]\\n  %classifier.1.weight[FLOAT, 1000x1280]\\n  %features.0.0.weight[FLOAT, 32x3x3x3]\\n  %features.0.1.bias[FLOAT, 32]\\n  %features.0.1.running_mean[FLOAT, 32]\\n  %features.0.1.running_var[FLOAT, 32]\\n  %features.0.1.weight[FLOAT, 32]\\n  %features.1.conv.0.0.weight[FLOAT, 32x1x3x3]\\n  %features.1.conv.0.1.bias[FLOAT, 32]\\n  %features.1.conv.0.1.running_mean[FLOAT, 32]\\n  %features.1.conv.0.1.running_var[FLOAT, 32]\\n  %features.1.conv.0.1.weight[FLOAT, 32]\\n  %features.1.conv.1.weight[FLOAT, 16x32x1x1]\\n  %features.1.conv.2.bias[FLOAT, 16]\\n  %features.1.conv.2.running_mean[FLOAT, 16]\\n  %features.1.conv.2.running_var[FLOAT, 16]\\n  %features.1.conv.2.weight[FLOAT, 16]\\n  %features.10.conv.0.0.weight[FLOAT, 384x64x1x1]\\n  %features.10.conv.0.1.bias[FLOAT, 384]\\n  %features.10.conv.0.1.running_mean[FLOAT, 384]\\n  %features.10.conv.0.1.running_var[FLOAT, 384]\\n  %features.10.conv.0.1.weight[FLOAT, 384]\\n  %features.10.conv.1.0.weight[FLOAT, 384x1x3x3]\\n  %features.10.conv.1.1.bias[FLOAT, 384]\\n  %features.10.conv.1.1.running_mean[FLOAT, 384]\\n  %features.10.conv.1.1.running_var[FLOAT, 384]\\n  %features.10.conv.1.1.weight[FLOAT, 384]\\n  %features.10.conv.2.weight[FLOAT, 64x384x1x1]\\n  %features.10.conv.3.bias[FLOAT, 64]\\n  %features.10.conv.3.running_mean[FLOAT, 64]\\n  %features.10.conv.3.running_var[FLOAT, 64]\\n  %features.10.conv.3.weight[FLOAT, 64]\\n  %features.11.conv.0.0.weight[FLOAT, 384x64x1x1]\\n  %features.11.conv.0.1.bias[FLOAT, 384]\\n  %features.11.conv.0.1.running_mean[FLOAT, 384]\\n  %features.11.conv.0.1.running_var[FLOAT, 384]\\n  %features.11.conv.0.1.weight[FLOAT, 384]\\n  %features.11.conv.1.0.weight[FLOAT, 384x1x3x3]\\n  %features.11.conv.1.1.bias[FLOAT, 384]\\n  %features.11.conv.1.1.running_mean[FLOAT, 384]\\n  %features.11.conv.1.1.running_var[FLOAT, 384]\\n  %features.11.conv.1.1.weight[FLOAT, 384]\\n  %features.11.conv.2.weight[FLOAT, 96x384x1x1]\\n  %features.11.conv.3.bias[FLOAT, 96]\\n  %features.11.conv.3.running_mean[FLOAT, 96]\\n  %features.11.conv.3.running_var[FLOAT, 96]\\n  %features.11.conv.3.weight[FLOAT, 96]\\n  %features.12.conv.0.0.weight[FLOAT, 576x96x1x1]\\n  %features.12.conv.0.1.bias[FLOAT, 576]\\n  %features.12.conv.0.1.running_mean[FLOAT, 576]\\n  %features.12.conv.0.1.running_var[FLOAT, 576]\\n  %features.12.conv.0.1.weight[FLOAT, 576]\\n  %features.12.conv.1.0.weight[FLOAT, 576x1x3x3]\\n  %features.12.conv.1.1.bias[FLOAT, 576]\\n  %features.12.conv.1.1.running_mean[FLOAT, 576]\\n  %features.12.conv.1.1.running_var[FLOAT, 576]\\n  %features.12.conv.1.1.weight[FLOAT, 576]\\n  %features.12.conv.2.weight[FLOAT, 96x576x1x1]\\n  %features.12.conv.3.bias[FLOAT, 96]\\n  %features.12.conv.3.running_mean[FLOAT, 96]\\n  %features.12.conv.3.running_var[FLOAT, 96]\\n  %features.12.conv.3.weight[FLOAT, 96]\\n  %features.13.conv.0.0.weight[FLOAT, 576x96x1x1]\\n  %features.13.conv.0.1.bias[FLOAT, 576]\\n  %features.13.conv.0.1.running_mean[FLOAT, 576]\\n  %features.13.conv.0.1.running_var[FLOAT, 576]\\n  %features.13.conv.0.1.weight[FLOAT, 576]\\n  %features.13.conv.1.0.weight[FLOAT, 576x1x3x3]\\n  %features.13.conv.1.1.bias[FLOAT, 576]\\n  %features.13.conv.1.1.running_mean[FLOAT, 576]\\n  %features.13.conv.1.1.running_var[FLOAT, 576]\\n  %features.13.conv.1.1.weight[FLOAT, 576]\\n  %features.13.conv.2.weight[FLOAT, 96x576x1x1]\\n  %features.13.conv.3.bias[FLOAT, 96]\\n  %features.13.conv.3.running_mean[FLOAT, 96]\\n  %features.13.conv.3.running_var[FLOAT, 96]\\n  %features.13.conv.3.weight[FLOAT, 96]\\n  %features.14.conv.0.0.weight[FLOAT, 576x96x1x1]\\n  %features.14.conv.0.1.bias[FLOAT, 576]\\n  %features.14.conv.0.1.running_mean[FLOAT, 576]\\n  %features.14.conv.0.1.running_var[FLOAT, 576]\\n  %features.14.conv.0.1.weight[FLOAT, 576]\\n  %features.14.conv.1.0.weight[FLOAT, 576x1x3x3]\\n  %features.14.conv.1.1.bias[FLOAT, 576]\\n  %features.14.conv.1.1.running_mean[FLOAT, 576]\\n  %features.14.conv.1.1.running_var[FLOAT, 576]\\n  %features.14.conv.1.1.weight[FLOAT, 576]\\n  %features.14.conv.2.weight[FLOAT, 160x576x1x1]\\n  %features.14.conv.3.bias[FLOAT, 160]\\n  %features.14.conv.3.running_mean[FLOAT, 160]\\n  %features.14.conv.3.running_var[FLOAT, 160]\\n  %features.14.conv.3.weight[FLOAT, 160]\\n  %features.15.conv.0.0.weight[FLOAT, 960x160x1x1]\\n  %features.15.conv.0.1.bias[FLOAT, 960]\\n  %features.15.conv.0.1.running_mean[FLOAT, 960]\\n  %features.15.conv.0.1.running_var[FLOAT, 960]\\n  %features.15.conv.0.1.weight[FLOAT, 960]\\n  %features.15.conv.1.0.weight[FLOAT, 960x1x3x3]\\n  %features.15.conv.1.1.bias[FLOAT, 960]\\n  %features.15.conv.1.1.running_mean[FLOAT, 960]\\n  %features.15.conv.1.1.running_var[FLOAT, 960]\\n  %features.15.conv.1.1.weight[FLOAT, 960]\\n  %features.15.conv.2.weight[FLOAT, 160x960x1x1]\\n  %features.15.conv.3.bias[FLOAT, 160]\\n  %features.15.conv.3.running_mean[FLOAT, 160]\\n  %features.15.conv.3.running_var[FLOAT, 160]\\n  %features.15.conv.3.weight[FLOAT, 160]\\n  %features.16.conv.0.0.weight[FLOAT, 960x160x1x1]\\n  %features.16.conv.0.1.bias[FLOAT, 960]\\n  %features.16.conv.0.1.running_mean[FLOAT, 960]\\n  %features.16.conv.0.1.running_var[FLOAT, 960]\\n  %features.16.conv.0.1.weight[FLOAT, 960]\\n  %features.16.conv.1.0.weight[FLOAT, 960x1x3x3]\\n  %features.16.conv.1.1.bias[FLOAT, 960]\\n  %features.16.conv.1.1.running_mean[FLOAT, 960]\\n  %features.16.conv.1.1.running_var[FLOAT, 960]\\n  %features.16.conv.1.1.weight[FLOAT, 960]\\n  %features.16.conv.2.weight[FLOAT, 160x960x1x1]\\n  %features.16.conv.3.bias[FLOAT, 160]\\n  %features.16.conv.3.running_mean[FLOAT, 160]\\n  %features.16.conv.3.running_var[FLOAT, 160]\\n  %features.16.conv.3.weight[FLOAT, 160]\\n  %features.17.conv.0.0.weight[FLOAT, 960x160x1x1]\\n  %features.17.conv.0.1.bias[FLOAT, 960]\\n  %features.17.conv.0.1.running_mean[FLOAT, 960]\\n  %features.17.conv.0.1.running_var[FLOAT, 960]\\n  %features.17.conv.0.1.weight[FLOAT, 960]\\n  %features.17.conv.1.0.weight[FLOAT, 960x1x3x3]\\n  %features.17.conv.1.1.bias[FLOAT, 960]\\n  %features.17.conv.1.1.running_mean[FLOAT, 960]\\n  %features.17.conv.1.1.running_var[FLOAT, 960]\\n  %features.17.conv.1.1.weight[FLOAT, 960]\\n  %features.17.conv.2.weight[FLOAT, 320x960x1x1]\\n  %features.17.conv.3.bias[FLOAT, 320]\\n  %features.17.conv.3.running_mean[FLOAT, 320]\\n  %features.17.conv.3.running_var[FLOAT, 320]\\n  %features.17.conv.3.weight[FLOAT, 320]\\n  %features.18.0.weight[FLOAT, 1280x320x1x1]\\n  %features.18.1.bias[FLOAT, 1280]\\n  %features.18.1.running_mean[FLOAT, 1280]\\n  %features.18.1.running_var[FLOAT, 1280]\\n  %features.18.1.weight[FLOAT, 1280]\\n  %features.2.conv.0.0.weight[FLOAT, 96x16x1x1]\\n  %features.2.conv.0.1.bias[FLOAT, 96]\\n  %features.2.conv.0.1.running_mean[FLOAT, 96]\\n  %features.2.conv.0.1.running_var[FLOAT, 96]\\n  %features.2.conv.0.1.weight[FLOAT, 96]\\n  %features.2.conv.1.0.weight[FLOAT, 96x1x3x3]\\n  %features.2.conv.1.1.bias[FLOAT, 96]\\n  %features.2.conv.1.1.running_mean[FLOAT, 96]\\n  %features.2.conv.1.1.running_var[FLOAT, 96]\\n  %features.2.conv.1.1.weight[FLOAT, 96]\\n  %features.2.conv.2.weight[FLOAT, 24x96x1x1]\\n  %features.2.conv.3.bias[FLOAT, 24]\\n  %features.2.conv.3.running_mean[FLOAT, 24]\\n  %features.2.conv.3.running_var[FLOAT, 24]\\n  %features.2.conv.3.weight[FLOAT, 24]\\n  %features.3.conv.0.0.weight[FLOAT, 144x24x1x1]\\n  %features.3.conv.0.1.bias[FLOAT, 144]\\n  %features.3.conv.0.1.running_mean[FLOAT, 144]\\n  %features.3.conv.0.1.running_var[FLOAT, 144]\\n  %features.3.conv.0.1.weight[FLOAT, 144]\\n  %features.3.conv.1.0.weight[FLOAT, 144x1x3x3]\\n  %features.3.conv.1.1.bias[FLOAT, 144]\\n  %features.3.conv.1.1.running_mean[FLOAT, 144]\\n  %features.3.conv.1.1.running_var[FLOAT, 144]\\n  %features.3.conv.1.1.weight[FLOAT, 144]\\n  %features.3.conv.2.weight[FLOAT, 24x144x1x1]\\n  %features.3.conv.3.bias[FLOAT, 24]\\n  %features.3.conv.3.running_mean[FLOAT, 24]\\n  %features.3.conv.3.running_var[FLOAT, 24]\\n  %features.3.conv.3.weight[FLOAT, 24]\\n  %features.4.conv.0.0.weight[FLOAT, 144x24x1x1]\\n  %features.4.conv.0.1.bias[FLOAT, 144]\\n  %features.4.conv.0.1.running_mean[FLOAT, 144]\\n  %features.4.conv.0.1.running_var[FLOAT, 144]\\n  %features.4.conv.0.1.weight[FLOAT, 144]\\n  %features.4.conv.1.0.weight[FLOAT, 144x1x3x3]\\n  %features.4.conv.1.1.bias[FLOAT, 144]\\n  %features.4.conv.1.1.running_mean[FLOAT, 144]\\n  %features.4.conv.1.1.running_var[FLOAT, 144]\\n  %features.4.conv.1.1.weight[FLOAT, 144]\\n  %features.4.conv.2.weight[FLOAT, 32x144x1x1]\\n  %features.4.conv.3.bias[FLOAT, 32]\\n  %features.4.conv.3.running_mean[FLOAT, 32]\\n  %features.4.conv.3.running_var[FLOAT, 32]\\n  %features.4.conv.3.weight[FLOAT, 32]\\n  %features.5.conv.0.0.weight[FLOAT, 192x32x1x1]\\n  %features.5.conv.0.1.bias[FLOAT, 192]\\n  %features.5.conv.0.1.running_mean[FLOAT, 192]\\n  %features.5.conv.0.1.running_var[FLOAT, 192]\\n  %features.5.conv.0.1.weight[FLOAT, 192]\\n  %features.5.conv.1.0.weight[FLOAT, 192x1x3x3]\\n  %features.5.conv.1.1.bias[FLOAT, 192]\\n  %features.5.conv.1.1.running_mean[FLOAT, 192]\\n  %features.5.conv.1.1.running_var[FLOAT, 192]\\n  %features.5.conv.1.1.weight[FLOAT, 192]\\n  %features.5.conv.2.weight[FLOAT, 32x192x1x1]\\n  %features.5.conv.3.bias[FLOAT, 32]\\n  %features.5.conv.3.running_mean[FLOAT, 32]\\n  %features.5.conv.3.running_var[FLOAT, 32]\\n  %features.5.conv.3.weight[FLOAT, 32]\\n  %features.6.conv.0.0.weight[FLOAT, 192x32x1x1]\\n  %features.6.conv.0.1.bias[FLOAT, 192]\\n  %features.6.conv.0.1.running_mean[FLOAT, 192]\\n  %features.6.conv.0.1.running_var[FLOAT, 192]\\n  %features.6.conv.0.1.weight[FLOAT, 192]\\n  %features.6.conv.1.0.weight[FLOAT, 192x1x3x3]\\n  %features.6.conv.1.1.bias[FLOAT, 192]\\n  %features.6.conv.1.1.running_mean[FLOAT, 192]\\n  %features.6.conv.1.1.running_var[FLOAT, 192]\\n  %features.6.conv.1.1.weight[FLOAT, 192]\\n  %features.6.conv.2.weight[FLOAT, 32x192x1x1]\\n  %features.6.conv.3.bias[FLOAT, 32]\\n  %features.6.conv.3.running_mean[FLOAT, 32]\\n  %features.6.conv.3.running_var[FLOAT, 32]\\n  %features.6.conv.3.weight[FLOAT, 32]\\n  %features.7.conv.0.0.weight[FLOAT, 192x32x1x1]\\n  %features.7.conv.0.1.bias[FLOAT, 192]\\n  %features.7.conv.0.1.running_mean[FLOAT, 192]\\n  %features.7.conv.0.1.running_var[FLOAT, 192]\\n  %features.7.conv.0.1.weight[FLOAT, 192]\\n  %features.7.conv.1.0.weight[FLOAT, 192x1x3x3]\\n  %features.7.conv.1.1.bias[FLOAT, 192]\\n  %features.7.conv.1.1.running_mean[FLOAT, 192]\\n  %features.7.conv.1.1.running_var[FLOAT, 192]\\n  %features.7.conv.1.1.weight[FLOAT, 192]\\n  %features.7.conv.2.weight[FLOAT, 64x192x1x1]\\n  %features.7.conv.3.bias[FLOAT, 64]\\n  %features.7.conv.3.running_mean[FLOAT, 64]\\n  %features.7.conv.3.running_var[FLOAT, 64]\\n  %features.7.conv.3.weight[FLOAT, 64]\\n  %features.8.conv.0.0.weight[FLOAT, 384x64x1x1]\\n  %features.8.conv.0.1.bias[FLOAT, 384]\\n  %features.8.conv.0.1.running_mean[FLOAT, 384]\\n  %features.8.conv.0.1.running_var[FLOAT, 384]\\n  %features.8.conv.0.1.weight[FLOAT, 384]\\n  %features.8.conv.1.0.weight[FLOAT, 384x1x3x3]\\n  %features.8.conv.1.1.bias[FLOAT, 384]\\n  %features.8.conv.1.1.running_mean[FLOAT, 384]\\n  %features.8.conv.1.1.running_var[FLOAT, 384]\\n  %features.8.conv.1.1.weight[FLOAT, 384]\\n  %features.8.conv.2.weight[FLOAT, 64x384x1x1]\\n  %features.8.conv.3.bias[FLOAT, 64]\\n  %features.8.conv.3.running_mean[FLOAT, 64]\\n  %features.8.conv.3.running_var[FLOAT, 64]\\n  %features.8.conv.3.weight[FLOAT, 64]\\n  %features.9.conv.0.0.weight[FLOAT, 384x64x1x1]\\n  %features.9.conv.0.1.bias[FLOAT, 384]\\n  %features.9.conv.0.1.running_mean[FLOAT, 384]\\n  %features.9.conv.0.1.running_var[FLOAT, 384]\\n  %features.9.conv.0.1.weight[FLOAT, 384]\\n  %features.9.conv.1.0.weight[FLOAT, 384x1x3x3]\\n  %features.9.conv.1.1.bias[FLOAT, 384]\\n  %features.9.conv.1.1.running_mean[FLOAT, 384]\\n  %features.9.conv.1.1.running_var[FLOAT, 384]\\n  %features.9.conv.1.1.weight[FLOAT, 384]\\n  %features.9.conv.2.weight[FLOAT, 64x384x1x1]\\n  %features.9.conv.3.bias[FLOAT, 64]\\n  %features.9.conv.3.running_mean[FLOAT, 64]\\n  %features.9.conv.3.running_var[FLOAT, 64]\\n  %features.9.conv.3.weight[FLOAT, 64]\\n) {\\n  %315 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%input, %features.0.0.weight)\\n  %316 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%315, %features.0.1.weight, %features.0.1.bias, %features.0.1.running_mean, %features.0.1.running_var)\\n  %317 = Constant[value = <Scalar Tensor []>]()\\n  %318 = Constant[value = <Scalar Tensor []>]()\\n  %319 = Clip(%316, %317, %318)\\n  %320 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%319, %features.1.conv.0.0.weight)\\n  %321 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%320, %features.1.conv.0.1.weight, %features.1.conv.0.1.bias, %features.1.conv.0.1.running_mean, %features.1.conv.0.1.running_var)\\n  %322 = Constant[value = <Scalar Tensor []>]()\\n  %323 = Constant[value = <Scalar Tensor []>]()\\n  %324 = Clip(%321, %322, %323)\\n  %325 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%324, %features.1.conv.1.weight)\\n  %326 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%325, %features.1.conv.2.weight, %features.1.conv.2.bias, %features.1.conv.2.running_mean, %features.1.conv.2.running_var)\\n  %327 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%326, %features.2.conv.0.0.weight)\\n  %328 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%327, %features.2.conv.0.1.weight, %features.2.conv.0.1.bias, %features.2.conv.0.1.running_mean, %features.2.conv.0.1.running_var)\\n  %329 = Constant[value = <Scalar Tensor []>]()\\n  %330 = Constant[value = <Scalar Tensor []>]()\\n  %331 = Clip(%328, %329, %330)\\n  %332 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%331, %features.2.conv.1.0.weight)\\n  %333 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%332, %features.2.conv.1.1.weight, %features.2.conv.1.1.bias, %features.2.conv.1.1.running_mean, %features.2.conv.1.1.running_var)\\n  %334 = Constant[value = <Scalar Tensor []>]()\\n  %335 = Constant[value = <Scalar Tensor []>]()\\n  %336 = Clip(%333, %334, %335)\\n  %337 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%336, %features.2.conv.2.weight)\\n  %338 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%337, %features.2.conv.3.weight, %features.2.conv.3.bias, %features.2.conv.3.running_mean, %features.2.conv.3.running_var)\\n  %339 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%338, %features.3.conv.0.0.weight)\\n  %340 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%339, %features.3.conv.0.1.weight, %features.3.conv.0.1.bias, %features.3.conv.0.1.running_mean, %features.3.conv.0.1.running_var)\\n  %341 = Constant[value = <Scalar Tensor []>]()\\n  %342 = Constant[value = <Scalar Tensor []>]()\\n  %343 = Clip(%340, %341, %342)\\n  %344 = Conv[dilations = [1, 1], group = 144, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%343, %features.3.conv.1.0.weight)\\n  %345 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%344, %features.3.conv.1.1.weight, %features.3.conv.1.1.bias, %features.3.conv.1.1.running_mean, %features.3.conv.1.1.running_var)\\n  %346 = Constant[value = <Scalar Tensor []>]()\\n  %347 = Constant[value = <Scalar Tensor []>]()\\n  %348 = Clip(%345, %346, %347)\\n  %349 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%348, %features.3.conv.2.weight)\\n  %350 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%349, %features.3.conv.3.weight, %features.3.conv.3.bias, %features.3.conv.3.running_mean, %features.3.conv.3.running_var)\\n  %351 = Add(%338, %350)\\n  %352 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%351, %features.4.conv.0.0.weight)\\n  %353 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%352, %features.4.conv.0.1.weight, %features.4.conv.0.1.bias, %features.4.conv.0.1.running_mean, %features.4.conv.0.1.running_var)\\n  %354 = Constant[value = <Scalar Tensor []>]()\\n  %355 = Constant[value = <Scalar Tensor []>]()\\n  %356 = Clip(%353, %354, %355)\\n  %357 = Conv[dilations = [1, 1], group = 144, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%356, %features.4.conv.1.0.weight)\\n  %358 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%357, %features.4.conv.1.1.weight, %features.4.conv.1.1.bias, %features.4.conv.1.1.running_mean, %features.4.conv.1.1.running_var)\\n  %359 = Constant[value = <Scalar Tensor []>]()\\n  %360 = Constant[value = <Scalar Tensor []>]()\\n  %361 = Clip(%358, %359, %360)\\n  %362 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%361, %features.4.conv.2.weight)\\n  %363 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%362, %features.4.conv.3.weight, %features.4.conv.3.bias, %features.4.conv.3.running_mean, %features.4.conv.3.running_var)\\n  %364 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%363, %features.5.conv.0.0.weight)\\n  %365 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%364, %features.5.conv.0.1.weight, %features.5.conv.0.1.bias, %features.5.conv.0.1.running_mean, %features.5.conv.0.1.running_var)\\n  %366 = Constant[value = <Scalar Tensor []>]()\\n  %367 = Constant[value = <Scalar Tensor []>]()\\n  %368 = Clip(%365, %366, %367)\\n  %369 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%368, %features.5.conv.1.0.weight)\\n  %370 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%369, %features.5.conv.1.1.weight, %features.5.conv.1.1.bias, %features.5.conv.1.1.running_mean, %features.5.conv.1.1.running_var)\\n  %371 = Constant[value = <Scalar Tensor []>]()\\n  %372 = Constant[value = <Scalar Tensor []>]()\\n  %373 = Clip(%370, %371, %372)\\n  %374 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%373, %features.5.conv.2.weight)\\n  %375 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%374, %features.5.conv.3.weight, %features.5.conv.3.bias, %features.5.conv.3.running_mean, %features.5.conv.3.running_var)\\n  %376 = Add(%363, %375)\\n  %377 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%376, %features.6.conv.0.0.weight)\\n  %378 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%377, %features.6.conv.0.1.weight, %features.6.conv.0.1.bias, %features.6.conv.0.1.running_mean, %features.6.conv.0.1.running_var)\\n  %379 = Constant[value = <Scalar Tensor []>]()\\n  %380 = Constant[value = <Scalar Tensor []>]()\\n  %381 = Clip(%378, %379, %380)\\n  %382 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%381, %features.6.conv.1.0.weight)\\n  %383 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%382, %features.6.conv.1.1.weight, %features.6.conv.1.1.bias, %features.6.conv.1.1.running_mean, %features.6.conv.1.1.running_var)\\n  %384 = Constant[value = <Scalar Tensor []>]()\\n  %385 = Constant[value = <Scalar Tensor []>]()\\n  %386 = Clip(%383, %384, %385)\\n  %387 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%386, %features.6.conv.2.weight)\\n  %388 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%387, %features.6.conv.3.weight, %features.6.conv.3.bias, %features.6.conv.3.running_mean, %features.6.conv.3.running_var)\\n  %389 = Add(%376, %388)\\n  %390 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%389, %features.7.conv.0.0.weight)\\n  %391 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%390, %features.7.conv.0.1.weight, %features.7.conv.0.1.bias, %features.7.conv.0.1.running_mean, %features.7.conv.0.1.running_var)\\n  %392 = Constant[value = <Scalar Tensor []>]()\\n  %393 = Constant[value = <Scalar Tensor []>]()\\n  %394 = Clip(%391, %392, %393)\\n  %395 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%394, %features.7.conv.1.0.weight)\\n  %396 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%395, %features.7.conv.1.1.weight, %features.7.conv.1.1.bias, %features.7.conv.1.1.running_mean, %features.7.conv.1.1.running_var)\\n  %397 = Constant[value = <Scalar Tensor []>]()\\n  %398 = Constant[value = <Scalar Tensor []>]()\\n  %399 = Clip(%396, %397, %398)\\n  %400 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%399, %features.7.conv.2.weight)\\n  %401 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%400, %features.7.conv.3.weight, %features.7.conv.3.bias, %features.7.conv.3.running_mean, %features.7.conv.3.running_var)\\n  %402 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%401, %features.8.conv.0.0.weight)\\n  %403 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%402, %features.8.conv.0.1.weight, %features.8.conv.0.1.bias, %features.8.conv.0.1.running_mean, %features.8.conv.0.1.running_var)\\n  %404 = Constant[value = <Scalar Tensor []>]()\\n  %405 = Constant[value = <Scalar Tensor []>]()\\n  %406 = Clip(%403, %404, %405)\\n  %407 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%406, %features.8.conv.1.0.weight)\\n  %408 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%407, %features.8.conv.1.1.weight, %features.8.conv.1.1.bias, %features.8.conv.1.1.running_mean, %features.8.conv.1.1.running_var)\\n  %409 = Constant[value = <Scalar Tensor []>]()\\n  %410 = Constant[value = <Scalar Tensor []>]()\\n  %411 = Clip(%408, %409, %410)\\n  %412 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%411, %features.8.conv.2.weight)\\n  %413 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%412, %features.8.conv.3.weight, %features.8.conv.3.bias, %features.8.conv.3.running_mean, %features.8.conv.3.running_var)\\n  %414 = Add(%401, %413)\\n  %415 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%414, %features.9.conv.0.0.weight)\\n  %416 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%415, %features.9.conv.0.1.weight, %features.9.conv.0.1.bias, %features.9.conv.0.1.running_mean, %features.9.conv.0.1.running_var)\\n  %417 = Constant[value = <Scalar Tensor []>]()\\n  %418 = Constant[value = <Scalar Tensor []>]()\\n  %419 = Clip(%416, %417, %418)\\n  %420 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%419, %features.9.conv.1.0.weight)\\n  %421 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%420, %features.9.conv.1.1.weight, %features.9.conv.1.1.bias, %features.9.conv.1.1.running_mean, %features.9.conv.1.1.running_var)\\n  %422 = Constant[value = <Scalar Tensor []>]()\\n  %423 = Constant[value = <Scalar Tensor []>]()\\n  %424 = Clip(%421, %422, %423)\\n  %425 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%424, %features.9.conv.2.weight)\\n  %426 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%425, %features.9.conv.3.weight, %features.9.conv.3.bias, %features.9.conv.3.running_mean, %features.9.conv.3.running_var)\\n  %427 = Add(%414, %426)\\n  %428 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%427, %features.10.conv.0.0.weight)\\n  %429 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%428, %features.10.conv.0.1.weight, %features.10.conv.0.1.bias, %features.10.conv.0.1.running_mean, %features.10.conv.0.1.running_var)\\n  %430 = Constant[value = <Scalar Tensor []>]()\\n  %431 = Constant[value = <Scalar Tensor []>]()\\n  %432 = Clip(%429, %430, %431)\\n  %433 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%432, %features.10.conv.1.0.weight)\\n  %434 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%433, %features.10.conv.1.1.weight, %features.10.conv.1.1.bias, %features.10.conv.1.1.running_mean, %features.10.conv.1.1.running_var)\\n  %435 = Constant[value = <Scalar Tensor []>]()\\n  %436 = Constant[value = <Scalar Tensor []>]()\\n  %437 = Clip(%434, %435, %436)\\n  %438 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%437, %features.10.conv.2.weight)\\n  %439 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%438, %features.10.conv.3.weight, %features.10.conv.3.bias, %features.10.conv.3.running_mean, %features.10.conv.3.running_var)\\n  %440 = Add(%427, %439)\\n  %441 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%440, %features.11.conv.0.0.weight)\\n  %442 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%441, %features.11.conv.0.1.weight, %features.11.conv.0.1.bias, %features.11.conv.0.1.running_mean, %features.11.conv.0.1.running_var)\\n  %443 = Constant[value = <Scalar Tensor []>]()\\n  %444 = Constant[value = <Scalar Tensor []>]()\\n  %445 = Clip(%442, %443, %444)\\n  %446 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%445, %features.11.conv.1.0.weight)\\n  %447 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%446, %features.11.conv.1.1.weight, %features.11.conv.1.1.bias, %features.11.conv.1.1.running_mean, %features.11.conv.1.1.running_var)\\n  %448 = Constant[value = <Scalar Tensor []>]()\\n  %449 = Constant[value = <Scalar Tensor []>]()\\n  %450 = Clip(%447, %448, %449)\\n  %451 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%450, %features.11.conv.2.weight)\\n  %452 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%451, %features.11.conv.3.weight, %features.11.conv.3.bias, %features.11.conv.3.running_mean, %features.11.conv.3.running_var)\\n  %453 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%452, %features.12.conv.0.0.weight)\\n  %454 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%453, %features.12.conv.0.1.weight, %features.12.conv.0.1.bias, %features.12.conv.0.1.running_mean, %features.12.conv.0.1.running_var)\\n  %455 = Constant[value = <Scalar Tensor []>]()\\n  %456 = Constant[value = <Scalar Tensor []>]()\\n  %457 = Clip(%454, %455, %456)\\n  %458 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%457, %features.12.conv.1.0.weight)\\n  %459 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%458, %features.12.conv.1.1.weight, %features.12.conv.1.1.bias, %features.12.conv.1.1.running_mean, %features.12.conv.1.1.running_var)\\n  %460 = Constant[value = <Scalar Tensor []>]()\\n  %461 = Constant[value = <Scalar Tensor []>]()\\n  %462 = Clip(%459, %460, %461)\\n  %463 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%462, %features.12.conv.2.weight)\\n  %464 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%463, %features.12.conv.3.weight, %features.12.conv.3.bias, %features.12.conv.3.running_mean, %features.12.conv.3.running_var)\\n  %465 = Add(%452, %464)\\n  %466 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%465, %features.13.conv.0.0.weight)\\n  %467 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%466, %features.13.conv.0.1.weight, %features.13.conv.0.1.bias, %features.13.conv.0.1.running_mean, %features.13.conv.0.1.running_var)\\n  %468 = Constant[value = <Scalar Tensor []>]()\\n  %469 = Constant[value = <Scalar Tensor []>]()\\n  %470 = Clip(%467, %468, %469)\\n  %471 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%470, %features.13.conv.1.0.weight)\\n  %472 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%471, %features.13.conv.1.1.weight, %features.13.conv.1.1.bias, %features.13.conv.1.1.running_mean, %features.13.conv.1.1.running_var)\\n  %473 = Constant[value = <Scalar Tensor []>]()\\n  %474 = Constant[value = <Scalar Tensor []>]()\\n  %475 = Clip(%472, %473, %474)\\n  %476 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%475, %features.13.conv.2.weight)\\n  %477 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%476, %features.13.conv.3.weight, %features.13.conv.3.bias, %features.13.conv.3.running_mean, %features.13.conv.3.running_var)\\n  %478 = Add(%465, %477)\\n  %479 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%478, %features.14.conv.0.0.weight)\\n  %480 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%479, %features.14.conv.0.1.weight, %features.14.conv.0.1.bias, %features.14.conv.0.1.running_mean, %features.14.conv.0.1.running_var)\\n  %481 = Constant[value = <Scalar Tensor []>]()\\n  %482 = Constant[value = <Scalar Tensor []>]()\\n  %483 = Clip(%480, %481, %482)\\n  %484 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%483, %features.14.conv.1.0.weight)\\n  %485 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%484, %features.14.conv.1.1.weight, %features.14.conv.1.1.bias, %features.14.conv.1.1.running_mean, %features.14.conv.1.1.running_var)\\n  %486 = Constant[value = <Scalar Tensor []>]()\\n  %487 = Constant[value = <Scalar Tensor []>]()\\n  %488 = Clip(%485, %486, %487)\\n  %489 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%488, %features.14.conv.2.weight)\\n  %490 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%489, %features.14.conv.3.weight, %features.14.conv.3.bias, %features.14.conv.3.running_mean, %features.14.conv.3.running_var)\\n  %491 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%490, %features.15.conv.0.0.weight)\\n  %492 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%491, %features.15.conv.0.1.weight, %features.15.conv.0.1.bias, %features.15.conv.0.1.running_mean, %features.15.conv.0.1.running_var)\\n  %493 = Constant[value = <Scalar Tensor []>]()\\n  %494 = Constant[value = <Scalar Tensor []>]()\\n  %495 = Clip(%492, %493, %494)\\n  %496 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%495, %features.15.conv.1.0.weight)\\n  %497 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%496, %features.15.conv.1.1.weight, %features.15.conv.1.1.bias, %features.15.conv.1.1.running_mean, %features.15.conv.1.1.running_var)\\n  %498 = Constant[value = <Scalar Tensor []>]()\\n  %499 = Constant[value = <Scalar Tensor []>]()\\n  %500 = Clip(%497, %498, %499)\\n  %501 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%500, %features.15.conv.2.weight)\\n  %502 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%501, %features.15.conv.3.weight, %features.15.conv.3.bias, %features.15.conv.3.running_mean, %features.15.conv.3.running_var)\\n  %503 = Add(%490, %502)\\n  %504 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%503, %features.16.conv.0.0.weight)\\n  %505 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%504, %features.16.conv.0.1.weight, %features.16.conv.0.1.bias, %features.16.conv.0.1.running_mean, %features.16.conv.0.1.running_var)\\n  %506 = Constant[value = <Scalar Tensor []>]()\\n  %507 = Constant[value = <Scalar Tensor []>]()\\n  %508 = Clip(%505, %506, %507)\\n  %509 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%508, %features.16.conv.1.0.weight)\\n  %510 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%509, %features.16.conv.1.1.weight, %features.16.conv.1.1.bias, %features.16.conv.1.1.running_mean, %features.16.conv.1.1.running_var)\\n  %511 = Constant[value = <Scalar Tensor []>]()\\n  %512 = Constant[value = <Scalar Tensor []>]()\\n  %513 = Clip(%510, %511, %512)\\n  %514 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%513, %features.16.conv.2.weight)\\n  %515 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%514, %features.16.conv.3.weight, %features.16.conv.3.bias, %features.16.conv.3.running_mean, %features.16.conv.3.running_var)\\n  %516 = Add(%503, %515)\\n  %517 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%516, %features.17.conv.0.0.weight)\\n  %518 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%517, %features.17.conv.0.1.weight, %features.17.conv.0.1.bias, %features.17.conv.0.1.running_mean, %features.17.conv.0.1.running_var)\\n  %519 = Constant[value = <Scalar Tensor []>]()\\n  %520 = Constant[value = <Scalar Tensor []>]()\\n  %521 = Clip(%518, %519, %520)\\n  %522 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%521, %features.17.conv.1.0.weight)\\n  %523 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%522, %features.17.conv.1.1.weight, %features.17.conv.1.1.bias, %features.17.conv.1.1.running_mean, %features.17.conv.1.1.running_var)\\n  %524 = Constant[value = <Scalar Tensor []>]()\\n  %525 = Constant[value = <Scalar Tensor []>]()\\n  %526 = Clip(%523, %524, %525)\\n  %527 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%526, %features.17.conv.2.weight)\\n  %528 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%527, %features.17.conv.3.weight, %features.17.conv.3.bias, %features.17.conv.3.running_mean, %features.17.conv.3.running_var)\\n  %529 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%528, %features.18.0.weight)\\n  %530 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%529, %features.18.1.weight, %features.18.1.bias, %features.18.1.running_mean, %features.18.1.running_var)\\n  %531 = Constant[value = <Scalar Tensor []>]()\\n  %532 = Constant[value = <Scalar Tensor []>]()\\n  %533 = Clip(%530, %531, %532)\\n  %534 = ReduceMean[axes = [2, 3], keepdims = 0](%533)\\n  %output = Gemm[alpha = 1, beta = 1, transB = 1](%534, %classifier.1.weight, %classifier.1.bias)\\n  return %output\\n}'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "dog_detection_model = onnx.load('dog-detection-model.onnx')\n",
    "onnx.checker.check_model(dog_detection_model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "onnx.helper.printable_graph(dog_detection_model.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dog Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_classification_model = torch.load('dog_classification_model.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=133, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_classification_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input to the model\n",
    "BATCH_SIZE =  1\n",
    "x = torch.randn(BATCH_SIZE, 3, 224, 224, requires_grad=True, device='cpu')\n",
    "output = dog_classification_model(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(dog_classification_model,\n",
    "                  x,\n",
    "                  \"dog-classification-model.onnx\",\n",
    "                  export_params=True,\n",
    "                  opset_version=11,\n",
    "                  do_constant_folding=True,\n",
    "                  input_names=['input'],\n",
    "                  output_names=['output']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph torch-jit-export (\\n  %input[FLOAT, 1x3x224x224]\\n) initializers (\\n  %classifier.1.bias[FLOAT, 133]\\n  %classifier.1.weight[FLOAT, 133x1280]\\n  %features.0.0.weight[FLOAT, 32x3x3x3]\\n  %features.0.1.bias[FLOAT, 32]\\n  %features.0.1.running_mean[FLOAT, 32]\\n  %features.0.1.running_var[FLOAT, 32]\\n  %features.0.1.weight[FLOAT, 32]\\n  %features.1.conv.0.0.weight[FLOAT, 32x1x3x3]\\n  %features.1.conv.0.1.bias[FLOAT, 32]\\n  %features.1.conv.0.1.running_mean[FLOAT, 32]\\n  %features.1.conv.0.1.running_var[FLOAT, 32]\\n  %features.1.conv.0.1.weight[FLOAT, 32]\\n  %features.1.conv.1.weight[FLOAT, 16x32x1x1]\\n  %features.1.conv.2.bias[FLOAT, 16]\\n  %features.1.conv.2.running_mean[FLOAT, 16]\\n  %features.1.conv.2.running_var[FLOAT, 16]\\n  %features.1.conv.2.weight[FLOAT, 16]\\n  %features.10.conv.0.0.weight[FLOAT, 384x64x1x1]\\n  %features.10.conv.0.1.bias[FLOAT, 384]\\n  %features.10.conv.0.1.running_mean[FLOAT, 384]\\n  %features.10.conv.0.1.running_var[FLOAT, 384]\\n  %features.10.conv.0.1.weight[FLOAT, 384]\\n  %features.10.conv.1.0.weight[FLOAT, 384x1x3x3]\\n  %features.10.conv.1.1.bias[FLOAT, 384]\\n  %features.10.conv.1.1.running_mean[FLOAT, 384]\\n  %features.10.conv.1.1.running_var[FLOAT, 384]\\n  %features.10.conv.1.1.weight[FLOAT, 384]\\n  %features.10.conv.2.weight[FLOAT, 64x384x1x1]\\n  %features.10.conv.3.bias[FLOAT, 64]\\n  %features.10.conv.3.running_mean[FLOAT, 64]\\n  %features.10.conv.3.running_var[FLOAT, 64]\\n  %features.10.conv.3.weight[FLOAT, 64]\\n  %features.11.conv.0.0.weight[FLOAT, 384x64x1x1]\\n  %features.11.conv.0.1.bias[FLOAT, 384]\\n  %features.11.conv.0.1.running_mean[FLOAT, 384]\\n  %features.11.conv.0.1.running_var[FLOAT, 384]\\n  %features.11.conv.0.1.weight[FLOAT, 384]\\n  %features.11.conv.1.0.weight[FLOAT, 384x1x3x3]\\n  %features.11.conv.1.1.bias[FLOAT, 384]\\n  %features.11.conv.1.1.running_mean[FLOAT, 384]\\n  %features.11.conv.1.1.running_var[FLOAT, 384]\\n  %features.11.conv.1.1.weight[FLOAT, 384]\\n  %features.11.conv.2.weight[FLOAT, 96x384x1x1]\\n  %features.11.conv.3.bias[FLOAT, 96]\\n  %features.11.conv.3.running_mean[FLOAT, 96]\\n  %features.11.conv.3.running_var[FLOAT, 96]\\n  %features.11.conv.3.weight[FLOAT, 96]\\n  %features.12.conv.0.0.weight[FLOAT, 576x96x1x1]\\n  %features.12.conv.0.1.bias[FLOAT, 576]\\n  %features.12.conv.0.1.running_mean[FLOAT, 576]\\n  %features.12.conv.0.1.running_var[FLOAT, 576]\\n  %features.12.conv.0.1.weight[FLOAT, 576]\\n  %features.12.conv.1.0.weight[FLOAT, 576x1x3x3]\\n  %features.12.conv.1.1.bias[FLOAT, 576]\\n  %features.12.conv.1.1.running_mean[FLOAT, 576]\\n  %features.12.conv.1.1.running_var[FLOAT, 576]\\n  %features.12.conv.1.1.weight[FLOAT, 576]\\n  %features.12.conv.2.weight[FLOAT, 96x576x1x1]\\n  %features.12.conv.3.bias[FLOAT, 96]\\n  %features.12.conv.3.running_mean[FLOAT, 96]\\n  %features.12.conv.3.running_var[FLOAT, 96]\\n  %features.12.conv.3.weight[FLOAT, 96]\\n  %features.13.conv.0.0.weight[FLOAT, 576x96x1x1]\\n  %features.13.conv.0.1.bias[FLOAT, 576]\\n  %features.13.conv.0.1.running_mean[FLOAT, 576]\\n  %features.13.conv.0.1.running_var[FLOAT, 576]\\n  %features.13.conv.0.1.weight[FLOAT, 576]\\n  %features.13.conv.1.0.weight[FLOAT, 576x1x3x3]\\n  %features.13.conv.1.1.bias[FLOAT, 576]\\n  %features.13.conv.1.1.running_mean[FLOAT, 576]\\n  %features.13.conv.1.1.running_var[FLOAT, 576]\\n  %features.13.conv.1.1.weight[FLOAT, 576]\\n  %features.13.conv.2.weight[FLOAT, 96x576x1x1]\\n  %features.13.conv.3.bias[FLOAT, 96]\\n  %features.13.conv.3.running_mean[FLOAT, 96]\\n  %features.13.conv.3.running_var[FLOAT, 96]\\n  %features.13.conv.3.weight[FLOAT, 96]\\n  %features.14.conv.0.0.weight[FLOAT, 576x96x1x1]\\n  %features.14.conv.0.1.bias[FLOAT, 576]\\n  %features.14.conv.0.1.running_mean[FLOAT, 576]\\n  %features.14.conv.0.1.running_var[FLOAT, 576]\\n  %features.14.conv.0.1.weight[FLOAT, 576]\\n  %features.14.conv.1.0.weight[FLOAT, 576x1x3x3]\\n  %features.14.conv.1.1.bias[FLOAT, 576]\\n  %features.14.conv.1.1.running_mean[FLOAT, 576]\\n  %features.14.conv.1.1.running_var[FLOAT, 576]\\n  %features.14.conv.1.1.weight[FLOAT, 576]\\n  %features.14.conv.2.weight[FLOAT, 160x576x1x1]\\n  %features.14.conv.3.bias[FLOAT, 160]\\n  %features.14.conv.3.running_mean[FLOAT, 160]\\n  %features.14.conv.3.running_var[FLOAT, 160]\\n  %features.14.conv.3.weight[FLOAT, 160]\\n  %features.15.conv.0.0.weight[FLOAT, 960x160x1x1]\\n  %features.15.conv.0.1.bias[FLOAT, 960]\\n  %features.15.conv.0.1.running_mean[FLOAT, 960]\\n  %features.15.conv.0.1.running_var[FLOAT, 960]\\n  %features.15.conv.0.1.weight[FLOAT, 960]\\n  %features.15.conv.1.0.weight[FLOAT, 960x1x3x3]\\n  %features.15.conv.1.1.bias[FLOAT, 960]\\n  %features.15.conv.1.1.running_mean[FLOAT, 960]\\n  %features.15.conv.1.1.running_var[FLOAT, 960]\\n  %features.15.conv.1.1.weight[FLOAT, 960]\\n  %features.15.conv.2.weight[FLOAT, 160x960x1x1]\\n  %features.15.conv.3.bias[FLOAT, 160]\\n  %features.15.conv.3.running_mean[FLOAT, 160]\\n  %features.15.conv.3.running_var[FLOAT, 160]\\n  %features.15.conv.3.weight[FLOAT, 160]\\n  %features.16.conv.0.0.weight[FLOAT, 960x160x1x1]\\n  %features.16.conv.0.1.bias[FLOAT, 960]\\n  %features.16.conv.0.1.running_mean[FLOAT, 960]\\n  %features.16.conv.0.1.running_var[FLOAT, 960]\\n  %features.16.conv.0.1.weight[FLOAT, 960]\\n  %features.16.conv.1.0.weight[FLOAT, 960x1x3x3]\\n  %features.16.conv.1.1.bias[FLOAT, 960]\\n  %features.16.conv.1.1.running_mean[FLOAT, 960]\\n  %features.16.conv.1.1.running_var[FLOAT, 960]\\n  %features.16.conv.1.1.weight[FLOAT, 960]\\n  %features.16.conv.2.weight[FLOAT, 160x960x1x1]\\n  %features.16.conv.3.bias[FLOAT, 160]\\n  %features.16.conv.3.running_mean[FLOAT, 160]\\n  %features.16.conv.3.running_var[FLOAT, 160]\\n  %features.16.conv.3.weight[FLOAT, 160]\\n  %features.17.conv.0.0.weight[FLOAT, 960x160x1x1]\\n  %features.17.conv.0.1.bias[FLOAT, 960]\\n  %features.17.conv.0.1.running_mean[FLOAT, 960]\\n  %features.17.conv.0.1.running_var[FLOAT, 960]\\n  %features.17.conv.0.1.weight[FLOAT, 960]\\n  %features.17.conv.1.0.weight[FLOAT, 960x1x3x3]\\n  %features.17.conv.1.1.bias[FLOAT, 960]\\n  %features.17.conv.1.1.running_mean[FLOAT, 960]\\n  %features.17.conv.1.1.running_var[FLOAT, 960]\\n  %features.17.conv.1.1.weight[FLOAT, 960]\\n  %features.17.conv.2.weight[FLOAT, 320x960x1x1]\\n  %features.17.conv.3.bias[FLOAT, 320]\\n  %features.17.conv.3.running_mean[FLOAT, 320]\\n  %features.17.conv.3.running_var[FLOAT, 320]\\n  %features.17.conv.3.weight[FLOAT, 320]\\n  %features.18.0.weight[FLOAT, 1280x320x1x1]\\n  %features.18.1.bias[FLOAT, 1280]\\n  %features.18.1.running_mean[FLOAT, 1280]\\n  %features.18.1.running_var[FLOAT, 1280]\\n  %features.18.1.weight[FLOAT, 1280]\\n  %features.2.conv.0.0.weight[FLOAT, 96x16x1x1]\\n  %features.2.conv.0.1.bias[FLOAT, 96]\\n  %features.2.conv.0.1.running_mean[FLOAT, 96]\\n  %features.2.conv.0.1.running_var[FLOAT, 96]\\n  %features.2.conv.0.1.weight[FLOAT, 96]\\n  %features.2.conv.1.0.weight[FLOAT, 96x1x3x3]\\n  %features.2.conv.1.1.bias[FLOAT, 96]\\n  %features.2.conv.1.1.running_mean[FLOAT, 96]\\n  %features.2.conv.1.1.running_var[FLOAT, 96]\\n  %features.2.conv.1.1.weight[FLOAT, 96]\\n  %features.2.conv.2.weight[FLOAT, 24x96x1x1]\\n  %features.2.conv.3.bias[FLOAT, 24]\\n  %features.2.conv.3.running_mean[FLOAT, 24]\\n  %features.2.conv.3.running_var[FLOAT, 24]\\n  %features.2.conv.3.weight[FLOAT, 24]\\n  %features.3.conv.0.0.weight[FLOAT, 144x24x1x1]\\n  %features.3.conv.0.1.bias[FLOAT, 144]\\n  %features.3.conv.0.1.running_mean[FLOAT, 144]\\n  %features.3.conv.0.1.running_var[FLOAT, 144]\\n  %features.3.conv.0.1.weight[FLOAT, 144]\\n  %features.3.conv.1.0.weight[FLOAT, 144x1x3x3]\\n  %features.3.conv.1.1.bias[FLOAT, 144]\\n  %features.3.conv.1.1.running_mean[FLOAT, 144]\\n  %features.3.conv.1.1.running_var[FLOAT, 144]\\n  %features.3.conv.1.1.weight[FLOAT, 144]\\n  %features.3.conv.2.weight[FLOAT, 24x144x1x1]\\n  %features.3.conv.3.bias[FLOAT, 24]\\n  %features.3.conv.3.running_mean[FLOAT, 24]\\n  %features.3.conv.3.running_var[FLOAT, 24]\\n  %features.3.conv.3.weight[FLOAT, 24]\\n  %features.4.conv.0.0.weight[FLOAT, 144x24x1x1]\\n  %features.4.conv.0.1.bias[FLOAT, 144]\\n  %features.4.conv.0.1.running_mean[FLOAT, 144]\\n  %features.4.conv.0.1.running_var[FLOAT, 144]\\n  %features.4.conv.0.1.weight[FLOAT, 144]\\n  %features.4.conv.1.0.weight[FLOAT, 144x1x3x3]\\n  %features.4.conv.1.1.bias[FLOAT, 144]\\n  %features.4.conv.1.1.running_mean[FLOAT, 144]\\n  %features.4.conv.1.1.running_var[FLOAT, 144]\\n  %features.4.conv.1.1.weight[FLOAT, 144]\\n  %features.4.conv.2.weight[FLOAT, 32x144x1x1]\\n  %features.4.conv.3.bias[FLOAT, 32]\\n  %features.4.conv.3.running_mean[FLOAT, 32]\\n  %features.4.conv.3.running_var[FLOAT, 32]\\n  %features.4.conv.3.weight[FLOAT, 32]\\n  %features.5.conv.0.0.weight[FLOAT, 192x32x1x1]\\n  %features.5.conv.0.1.bias[FLOAT, 192]\\n  %features.5.conv.0.1.running_mean[FLOAT, 192]\\n  %features.5.conv.0.1.running_var[FLOAT, 192]\\n  %features.5.conv.0.1.weight[FLOAT, 192]\\n  %features.5.conv.1.0.weight[FLOAT, 192x1x3x3]\\n  %features.5.conv.1.1.bias[FLOAT, 192]\\n  %features.5.conv.1.1.running_mean[FLOAT, 192]\\n  %features.5.conv.1.1.running_var[FLOAT, 192]\\n  %features.5.conv.1.1.weight[FLOAT, 192]\\n  %features.5.conv.2.weight[FLOAT, 32x192x1x1]\\n  %features.5.conv.3.bias[FLOAT, 32]\\n  %features.5.conv.3.running_mean[FLOAT, 32]\\n  %features.5.conv.3.running_var[FLOAT, 32]\\n  %features.5.conv.3.weight[FLOAT, 32]\\n  %features.6.conv.0.0.weight[FLOAT, 192x32x1x1]\\n  %features.6.conv.0.1.bias[FLOAT, 192]\\n  %features.6.conv.0.1.running_mean[FLOAT, 192]\\n  %features.6.conv.0.1.running_var[FLOAT, 192]\\n  %features.6.conv.0.1.weight[FLOAT, 192]\\n  %features.6.conv.1.0.weight[FLOAT, 192x1x3x3]\\n  %features.6.conv.1.1.bias[FLOAT, 192]\\n  %features.6.conv.1.1.running_mean[FLOAT, 192]\\n  %features.6.conv.1.1.running_var[FLOAT, 192]\\n  %features.6.conv.1.1.weight[FLOAT, 192]\\n  %features.6.conv.2.weight[FLOAT, 32x192x1x1]\\n  %features.6.conv.3.bias[FLOAT, 32]\\n  %features.6.conv.3.running_mean[FLOAT, 32]\\n  %features.6.conv.3.running_var[FLOAT, 32]\\n  %features.6.conv.3.weight[FLOAT, 32]\\n  %features.7.conv.0.0.weight[FLOAT, 192x32x1x1]\\n  %features.7.conv.0.1.bias[FLOAT, 192]\\n  %features.7.conv.0.1.running_mean[FLOAT, 192]\\n  %features.7.conv.0.1.running_var[FLOAT, 192]\\n  %features.7.conv.0.1.weight[FLOAT, 192]\\n  %features.7.conv.1.0.weight[FLOAT, 192x1x3x3]\\n  %features.7.conv.1.1.bias[FLOAT, 192]\\n  %features.7.conv.1.1.running_mean[FLOAT, 192]\\n  %features.7.conv.1.1.running_var[FLOAT, 192]\\n  %features.7.conv.1.1.weight[FLOAT, 192]\\n  %features.7.conv.2.weight[FLOAT, 64x192x1x1]\\n  %features.7.conv.3.bias[FLOAT, 64]\\n  %features.7.conv.3.running_mean[FLOAT, 64]\\n  %features.7.conv.3.running_var[FLOAT, 64]\\n  %features.7.conv.3.weight[FLOAT, 64]\\n  %features.8.conv.0.0.weight[FLOAT, 384x64x1x1]\\n  %features.8.conv.0.1.bias[FLOAT, 384]\\n  %features.8.conv.0.1.running_mean[FLOAT, 384]\\n  %features.8.conv.0.1.running_var[FLOAT, 384]\\n  %features.8.conv.0.1.weight[FLOAT, 384]\\n  %features.8.conv.1.0.weight[FLOAT, 384x1x3x3]\\n  %features.8.conv.1.1.bias[FLOAT, 384]\\n  %features.8.conv.1.1.running_mean[FLOAT, 384]\\n  %features.8.conv.1.1.running_var[FLOAT, 384]\\n  %features.8.conv.1.1.weight[FLOAT, 384]\\n  %features.8.conv.2.weight[FLOAT, 64x384x1x1]\\n  %features.8.conv.3.bias[FLOAT, 64]\\n  %features.8.conv.3.running_mean[FLOAT, 64]\\n  %features.8.conv.3.running_var[FLOAT, 64]\\n  %features.8.conv.3.weight[FLOAT, 64]\\n  %features.9.conv.0.0.weight[FLOAT, 384x64x1x1]\\n  %features.9.conv.0.1.bias[FLOAT, 384]\\n  %features.9.conv.0.1.running_mean[FLOAT, 384]\\n  %features.9.conv.0.1.running_var[FLOAT, 384]\\n  %features.9.conv.0.1.weight[FLOAT, 384]\\n  %features.9.conv.1.0.weight[FLOAT, 384x1x3x3]\\n  %features.9.conv.1.1.bias[FLOAT, 384]\\n  %features.9.conv.1.1.running_mean[FLOAT, 384]\\n  %features.9.conv.1.1.running_var[FLOAT, 384]\\n  %features.9.conv.1.1.weight[FLOAT, 384]\\n  %features.9.conv.2.weight[FLOAT, 64x384x1x1]\\n  %features.9.conv.3.bias[FLOAT, 64]\\n  %features.9.conv.3.running_mean[FLOAT, 64]\\n  %features.9.conv.3.running_var[FLOAT, 64]\\n  %features.9.conv.3.weight[FLOAT, 64]\\n) {\\n  %315 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%input, %features.0.0.weight)\\n  %316 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%315, %features.0.1.weight, %features.0.1.bias, %features.0.1.running_mean, %features.0.1.running_var)\\n  %317 = Constant[value = <Scalar Tensor []>]()\\n  %318 = Constant[value = <Scalar Tensor []>]()\\n  %319 = Clip(%316, %317, %318)\\n  %320 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%319, %features.1.conv.0.0.weight)\\n  %321 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%320, %features.1.conv.0.1.weight, %features.1.conv.0.1.bias, %features.1.conv.0.1.running_mean, %features.1.conv.0.1.running_var)\\n  %322 = Constant[value = <Scalar Tensor []>]()\\n  %323 = Constant[value = <Scalar Tensor []>]()\\n  %324 = Clip(%321, %322, %323)\\n  %325 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%324, %features.1.conv.1.weight)\\n  %326 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%325, %features.1.conv.2.weight, %features.1.conv.2.bias, %features.1.conv.2.running_mean, %features.1.conv.2.running_var)\\n  %327 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%326, %features.2.conv.0.0.weight)\\n  %328 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%327, %features.2.conv.0.1.weight, %features.2.conv.0.1.bias, %features.2.conv.0.1.running_mean, %features.2.conv.0.1.running_var)\\n  %329 = Constant[value = <Scalar Tensor []>]()\\n  %330 = Constant[value = <Scalar Tensor []>]()\\n  %331 = Clip(%328, %329, %330)\\n  %332 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%331, %features.2.conv.1.0.weight)\\n  %333 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%332, %features.2.conv.1.1.weight, %features.2.conv.1.1.bias, %features.2.conv.1.1.running_mean, %features.2.conv.1.1.running_var)\\n  %334 = Constant[value = <Scalar Tensor []>]()\\n  %335 = Constant[value = <Scalar Tensor []>]()\\n  %336 = Clip(%333, %334, %335)\\n  %337 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%336, %features.2.conv.2.weight)\\n  %338 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%337, %features.2.conv.3.weight, %features.2.conv.3.bias, %features.2.conv.3.running_mean, %features.2.conv.3.running_var)\\n  %339 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%338, %features.3.conv.0.0.weight)\\n  %340 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%339, %features.3.conv.0.1.weight, %features.3.conv.0.1.bias, %features.3.conv.0.1.running_mean, %features.3.conv.0.1.running_var)\\n  %341 = Constant[value = <Scalar Tensor []>]()\\n  %342 = Constant[value = <Scalar Tensor []>]()\\n  %343 = Clip(%340, %341, %342)\\n  %344 = Conv[dilations = [1, 1], group = 144, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%343, %features.3.conv.1.0.weight)\\n  %345 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%344, %features.3.conv.1.1.weight, %features.3.conv.1.1.bias, %features.3.conv.1.1.running_mean, %features.3.conv.1.1.running_var)\\n  %346 = Constant[value = <Scalar Tensor []>]()\\n  %347 = Constant[value = <Scalar Tensor []>]()\\n  %348 = Clip(%345, %346, %347)\\n  %349 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%348, %features.3.conv.2.weight)\\n  %350 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%349, %features.3.conv.3.weight, %features.3.conv.3.bias, %features.3.conv.3.running_mean, %features.3.conv.3.running_var)\\n  %351 = Add(%338, %350)\\n  %352 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%351, %features.4.conv.0.0.weight)\\n  %353 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%352, %features.4.conv.0.1.weight, %features.4.conv.0.1.bias, %features.4.conv.0.1.running_mean, %features.4.conv.0.1.running_var)\\n  %354 = Constant[value = <Scalar Tensor []>]()\\n  %355 = Constant[value = <Scalar Tensor []>]()\\n  %356 = Clip(%353, %354, %355)\\n  %357 = Conv[dilations = [1, 1], group = 144, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%356, %features.4.conv.1.0.weight)\\n  %358 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%357, %features.4.conv.1.1.weight, %features.4.conv.1.1.bias, %features.4.conv.1.1.running_mean, %features.4.conv.1.1.running_var)\\n  %359 = Constant[value = <Scalar Tensor []>]()\\n  %360 = Constant[value = <Scalar Tensor []>]()\\n  %361 = Clip(%358, %359, %360)\\n  %362 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%361, %features.4.conv.2.weight)\\n  %363 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%362, %features.4.conv.3.weight, %features.4.conv.3.bias, %features.4.conv.3.running_mean, %features.4.conv.3.running_var)\\n  %364 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%363, %features.5.conv.0.0.weight)\\n  %365 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%364, %features.5.conv.0.1.weight, %features.5.conv.0.1.bias, %features.5.conv.0.1.running_mean, %features.5.conv.0.1.running_var)\\n  %366 = Constant[value = <Scalar Tensor []>]()\\n  %367 = Constant[value = <Scalar Tensor []>]()\\n  %368 = Clip(%365, %366, %367)\\n  %369 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%368, %features.5.conv.1.0.weight)\\n  %370 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%369, %features.5.conv.1.1.weight, %features.5.conv.1.1.bias, %features.5.conv.1.1.running_mean, %features.5.conv.1.1.running_var)\\n  %371 = Constant[value = <Scalar Tensor []>]()\\n  %372 = Constant[value = <Scalar Tensor []>]()\\n  %373 = Clip(%370, %371, %372)\\n  %374 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%373, %features.5.conv.2.weight)\\n  %375 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%374, %features.5.conv.3.weight, %features.5.conv.3.bias, %features.5.conv.3.running_mean, %features.5.conv.3.running_var)\\n  %376 = Add(%363, %375)\\n  %377 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%376, %features.6.conv.0.0.weight)\\n  %378 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%377, %features.6.conv.0.1.weight, %features.6.conv.0.1.bias, %features.6.conv.0.1.running_mean, %features.6.conv.0.1.running_var)\\n  %379 = Constant[value = <Scalar Tensor []>]()\\n  %380 = Constant[value = <Scalar Tensor []>]()\\n  %381 = Clip(%378, %379, %380)\\n  %382 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%381, %features.6.conv.1.0.weight)\\n  %383 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%382, %features.6.conv.1.1.weight, %features.6.conv.1.1.bias, %features.6.conv.1.1.running_mean, %features.6.conv.1.1.running_var)\\n  %384 = Constant[value = <Scalar Tensor []>]()\\n  %385 = Constant[value = <Scalar Tensor []>]()\\n  %386 = Clip(%383, %384, %385)\\n  %387 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%386, %features.6.conv.2.weight)\\n  %388 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%387, %features.6.conv.3.weight, %features.6.conv.3.bias, %features.6.conv.3.running_mean, %features.6.conv.3.running_var)\\n  %389 = Add(%376, %388)\\n  %390 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%389, %features.7.conv.0.0.weight)\\n  %391 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%390, %features.7.conv.0.1.weight, %features.7.conv.0.1.bias, %features.7.conv.0.1.running_mean, %features.7.conv.0.1.running_var)\\n  %392 = Constant[value = <Scalar Tensor []>]()\\n  %393 = Constant[value = <Scalar Tensor []>]()\\n  %394 = Clip(%391, %392, %393)\\n  %395 = Conv[dilations = [1, 1], group = 192, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%394, %features.7.conv.1.0.weight)\\n  %396 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%395, %features.7.conv.1.1.weight, %features.7.conv.1.1.bias, %features.7.conv.1.1.running_mean, %features.7.conv.1.1.running_var)\\n  %397 = Constant[value = <Scalar Tensor []>]()\\n  %398 = Constant[value = <Scalar Tensor []>]()\\n  %399 = Clip(%396, %397, %398)\\n  %400 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%399, %features.7.conv.2.weight)\\n  %401 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%400, %features.7.conv.3.weight, %features.7.conv.3.bias, %features.7.conv.3.running_mean, %features.7.conv.3.running_var)\\n  %402 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%401, %features.8.conv.0.0.weight)\\n  %403 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%402, %features.8.conv.0.1.weight, %features.8.conv.0.1.bias, %features.8.conv.0.1.running_mean, %features.8.conv.0.1.running_var)\\n  %404 = Constant[value = <Scalar Tensor []>]()\\n  %405 = Constant[value = <Scalar Tensor []>]()\\n  %406 = Clip(%403, %404, %405)\\n  %407 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%406, %features.8.conv.1.0.weight)\\n  %408 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%407, %features.8.conv.1.1.weight, %features.8.conv.1.1.bias, %features.8.conv.1.1.running_mean, %features.8.conv.1.1.running_var)\\n  %409 = Constant[value = <Scalar Tensor []>]()\\n  %410 = Constant[value = <Scalar Tensor []>]()\\n  %411 = Clip(%408, %409, %410)\\n  %412 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%411, %features.8.conv.2.weight)\\n  %413 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%412, %features.8.conv.3.weight, %features.8.conv.3.bias, %features.8.conv.3.running_mean, %features.8.conv.3.running_var)\\n  %414 = Add(%401, %413)\\n  %415 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%414, %features.9.conv.0.0.weight)\\n  %416 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%415, %features.9.conv.0.1.weight, %features.9.conv.0.1.bias, %features.9.conv.0.1.running_mean, %features.9.conv.0.1.running_var)\\n  %417 = Constant[value = <Scalar Tensor []>]()\\n  %418 = Constant[value = <Scalar Tensor []>]()\\n  %419 = Clip(%416, %417, %418)\\n  %420 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%419, %features.9.conv.1.0.weight)\\n  %421 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%420, %features.9.conv.1.1.weight, %features.9.conv.1.1.bias, %features.9.conv.1.1.running_mean, %features.9.conv.1.1.running_var)\\n  %422 = Constant[value = <Scalar Tensor []>]()\\n  %423 = Constant[value = <Scalar Tensor []>]()\\n  %424 = Clip(%421, %422, %423)\\n  %425 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%424, %features.9.conv.2.weight)\\n  %426 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%425, %features.9.conv.3.weight, %features.9.conv.3.bias, %features.9.conv.3.running_mean, %features.9.conv.3.running_var)\\n  %427 = Add(%414, %426)\\n  %428 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%427, %features.10.conv.0.0.weight)\\n  %429 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%428, %features.10.conv.0.1.weight, %features.10.conv.0.1.bias, %features.10.conv.0.1.running_mean, %features.10.conv.0.1.running_var)\\n  %430 = Constant[value = <Scalar Tensor []>]()\\n  %431 = Constant[value = <Scalar Tensor []>]()\\n  %432 = Clip(%429, %430, %431)\\n  %433 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%432, %features.10.conv.1.0.weight)\\n  %434 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%433, %features.10.conv.1.1.weight, %features.10.conv.1.1.bias, %features.10.conv.1.1.running_mean, %features.10.conv.1.1.running_var)\\n  %435 = Constant[value = <Scalar Tensor []>]()\\n  %436 = Constant[value = <Scalar Tensor []>]()\\n  %437 = Clip(%434, %435, %436)\\n  %438 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%437, %features.10.conv.2.weight)\\n  %439 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%438, %features.10.conv.3.weight, %features.10.conv.3.bias, %features.10.conv.3.running_mean, %features.10.conv.3.running_var)\\n  %440 = Add(%427, %439)\\n  %441 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%440, %features.11.conv.0.0.weight)\\n  %442 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%441, %features.11.conv.0.1.weight, %features.11.conv.0.1.bias, %features.11.conv.0.1.running_mean, %features.11.conv.0.1.running_var)\\n  %443 = Constant[value = <Scalar Tensor []>]()\\n  %444 = Constant[value = <Scalar Tensor []>]()\\n  %445 = Clip(%442, %443, %444)\\n  %446 = Conv[dilations = [1, 1], group = 384, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%445, %features.11.conv.1.0.weight)\\n  %447 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%446, %features.11.conv.1.1.weight, %features.11.conv.1.1.bias, %features.11.conv.1.1.running_mean, %features.11.conv.1.1.running_var)\\n  %448 = Constant[value = <Scalar Tensor []>]()\\n  %449 = Constant[value = <Scalar Tensor []>]()\\n  %450 = Clip(%447, %448, %449)\\n  %451 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%450, %features.11.conv.2.weight)\\n  %452 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%451, %features.11.conv.3.weight, %features.11.conv.3.bias, %features.11.conv.3.running_mean, %features.11.conv.3.running_var)\\n  %453 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%452, %features.12.conv.0.0.weight)\\n  %454 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%453, %features.12.conv.0.1.weight, %features.12.conv.0.1.bias, %features.12.conv.0.1.running_mean, %features.12.conv.0.1.running_var)\\n  %455 = Constant[value = <Scalar Tensor []>]()\\n  %456 = Constant[value = <Scalar Tensor []>]()\\n  %457 = Clip(%454, %455, %456)\\n  %458 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%457, %features.12.conv.1.0.weight)\\n  %459 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%458, %features.12.conv.1.1.weight, %features.12.conv.1.1.bias, %features.12.conv.1.1.running_mean, %features.12.conv.1.1.running_var)\\n  %460 = Constant[value = <Scalar Tensor []>]()\\n  %461 = Constant[value = <Scalar Tensor []>]()\\n  %462 = Clip(%459, %460, %461)\\n  %463 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%462, %features.12.conv.2.weight)\\n  %464 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%463, %features.12.conv.3.weight, %features.12.conv.3.bias, %features.12.conv.3.running_mean, %features.12.conv.3.running_var)\\n  %465 = Add(%452, %464)\\n  %466 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%465, %features.13.conv.0.0.weight)\\n  %467 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%466, %features.13.conv.0.1.weight, %features.13.conv.0.1.bias, %features.13.conv.0.1.running_mean, %features.13.conv.0.1.running_var)\\n  %468 = Constant[value = <Scalar Tensor []>]()\\n  %469 = Constant[value = <Scalar Tensor []>]()\\n  %470 = Clip(%467, %468, %469)\\n  %471 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%470, %features.13.conv.1.0.weight)\\n  %472 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%471, %features.13.conv.1.1.weight, %features.13.conv.1.1.bias, %features.13.conv.1.1.running_mean, %features.13.conv.1.1.running_var)\\n  %473 = Constant[value = <Scalar Tensor []>]()\\n  %474 = Constant[value = <Scalar Tensor []>]()\\n  %475 = Clip(%472, %473, %474)\\n  %476 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%475, %features.13.conv.2.weight)\\n  %477 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%476, %features.13.conv.3.weight, %features.13.conv.3.bias, %features.13.conv.3.running_mean, %features.13.conv.3.running_var)\\n  %478 = Add(%465, %477)\\n  %479 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%478, %features.14.conv.0.0.weight)\\n  %480 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%479, %features.14.conv.0.1.weight, %features.14.conv.0.1.bias, %features.14.conv.0.1.running_mean, %features.14.conv.0.1.running_var)\\n  %481 = Constant[value = <Scalar Tensor []>]()\\n  %482 = Constant[value = <Scalar Tensor []>]()\\n  %483 = Clip(%480, %481, %482)\\n  %484 = Conv[dilations = [1, 1], group = 576, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [2, 2]](%483, %features.14.conv.1.0.weight)\\n  %485 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%484, %features.14.conv.1.1.weight, %features.14.conv.1.1.bias, %features.14.conv.1.1.running_mean, %features.14.conv.1.1.running_var)\\n  %486 = Constant[value = <Scalar Tensor []>]()\\n  %487 = Constant[value = <Scalar Tensor []>]()\\n  %488 = Clip(%485, %486, %487)\\n  %489 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%488, %features.14.conv.2.weight)\\n  %490 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%489, %features.14.conv.3.weight, %features.14.conv.3.bias, %features.14.conv.3.running_mean, %features.14.conv.3.running_var)\\n  %491 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%490, %features.15.conv.0.0.weight)\\n  %492 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%491, %features.15.conv.0.1.weight, %features.15.conv.0.1.bias, %features.15.conv.0.1.running_mean, %features.15.conv.0.1.running_var)\\n  %493 = Constant[value = <Scalar Tensor []>]()\\n  %494 = Constant[value = <Scalar Tensor []>]()\\n  %495 = Clip(%492, %493, %494)\\n  %496 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%495, %features.15.conv.1.0.weight)\\n  %497 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%496, %features.15.conv.1.1.weight, %features.15.conv.1.1.bias, %features.15.conv.1.1.running_mean, %features.15.conv.1.1.running_var)\\n  %498 = Constant[value = <Scalar Tensor []>]()\\n  %499 = Constant[value = <Scalar Tensor []>]()\\n  %500 = Clip(%497, %498, %499)\\n  %501 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%500, %features.15.conv.2.weight)\\n  %502 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%501, %features.15.conv.3.weight, %features.15.conv.3.bias, %features.15.conv.3.running_mean, %features.15.conv.3.running_var)\\n  %503 = Add(%490, %502)\\n  %504 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%503, %features.16.conv.0.0.weight)\\n  %505 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%504, %features.16.conv.0.1.weight, %features.16.conv.0.1.bias, %features.16.conv.0.1.running_mean, %features.16.conv.0.1.running_var)\\n  %506 = Constant[value = <Scalar Tensor []>]()\\n  %507 = Constant[value = <Scalar Tensor []>]()\\n  %508 = Clip(%505, %506, %507)\\n  %509 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%508, %features.16.conv.1.0.weight)\\n  %510 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%509, %features.16.conv.1.1.weight, %features.16.conv.1.1.bias, %features.16.conv.1.1.running_mean, %features.16.conv.1.1.running_var)\\n  %511 = Constant[value = <Scalar Tensor []>]()\\n  %512 = Constant[value = <Scalar Tensor []>]()\\n  %513 = Clip(%510, %511, %512)\\n  %514 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%513, %features.16.conv.2.weight)\\n  %515 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%514, %features.16.conv.3.weight, %features.16.conv.3.bias, %features.16.conv.3.running_mean, %features.16.conv.3.running_var)\\n  %516 = Add(%503, %515)\\n  %517 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%516, %features.17.conv.0.0.weight)\\n  %518 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%517, %features.17.conv.0.1.weight, %features.17.conv.0.1.bias, %features.17.conv.0.1.running_mean, %features.17.conv.0.1.running_var)\\n  %519 = Constant[value = <Scalar Tensor []>]()\\n  %520 = Constant[value = <Scalar Tensor []>]()\\n  %521 = Clip(%518, %519, %520)\\n  %522 = Conv[dilations = [1, 1], group = 960, kernel_shape = [3, 3], pads = [1, 1, 1, 1], strides = [1, 1]](%521, %features.17.conv.1.0.weight)\\n  %523 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%522, %features.17.conv.1.1.weight, %features.17.conv.1.1.bias, %features.17.conv.1.1.running_mean, %features.17.conv.1.1.running_var)\\n  %524 = Constant[value = <Scalar Tensor []>]()\\n  %525 = Constant[value = <Scalar Tensor []>]()\\n  %526 = Clip(%523, %524, %525)\\n  %527 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%526, %features.17.conv.2.weight)\\n  %528 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%527, %features.17.conv.3.weight, %features.17.conv.3.bias, %features.17.conv.3.running_mean, %features.17.conv.3.running_var)\\n  %529 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%528, %features.18.0.weight)\\n  %530 = BatchNormalization[epsilon = 9.99999974737875e-06, momentum = 0.899999976158142](%529, %features.18.1.weight, %features.18.1.bias, %features.18.1.running_mean, %features.18.1.running_var)\\n  %531 = Constant[value = <Scalar Tensor []>]()\\n  %532 = Constant[value = <Scalar Tensor []>]()\\n  %533 = Clip(%530, %531, %532)\\n  %534 = ReduceMean[axes = [2, 3], keepdims = 0](%533)\\n  %output = Gemm[alpha = 1, beta = 1, transB = 1](%534, %classifier.1.weight, %classifier.1.bias)\\n  return %output\\n}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "dog_classification_model = onnx.load('dog-classification-model.onnx')\n",
    "onnx.checker.check_model(dog_classification_model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "onnx.helper.printable_graph(dog_classification_model.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
